diff --git a/encoder.py b/encoder.py
index 20a6efb..33bd332 100644
--- a/encoder.py
+++ b/encoder.py
@@ -41,14 +41,17 @@ class Encoder(nn.Module):
 
         self.transformerLayer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=d_inner, batch_first=True)
         self.transformer = nn.TransformerEncoder(self.transformerLayer, num_layers=n_layers)
-
+        self.layer_stack = nn.ModuleList([
+                    EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)
+                    for _ in range(n_layers)])
 
     def forward(self, src_seq, src_pos):
 
         enc_output = self.src_emb(src_seq)
         enc_output += self.position_enc(src_pos)
-
-        enc_output = self.transformer(enc_output)
+        for enc_layer in self.layer_stack:
+            enc_output,  = enc_layer(enc_output)
+        # enc_output = self.transformer(enc_output)
 
         return enc_output,
 
diff --git a/evaluation/evaluate.py b/evaluation/evaluate.py
index a3ccd30..05f8113 100644
--- a/evaluation/evaluate.py
+++ b/evaluation/evaluate.py
@@ -20,7 +20,6 @@ def test_log(model, data_dir, device):
 
     test_size = len(paths)
 
-
     with torch.no_grad():
 
         for path in paths:
diff --git a/wandb/latest-run b/wandb/latest-run
index aa30e10..d42da04 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210928_001712-1y07wtlj
\ No newline at end of file
+run-20210928_102928-qzj03d7y
\ No newline at end of file
